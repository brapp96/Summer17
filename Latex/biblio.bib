 
@article{DSD,
    abstract = {In protein-protein interaction (PPI) networks, functional similarity is often inferred based on the function of directly interacting proteins, or more generally, some notion of interaction network proximity among proteins in a local neighborhood. Prior methods typically measure proximity as the shortest-path distance in the network, but this has only a limited ability to capture fine-grained neighborhood distinctions, because most proteins are close to each other, and there are many ties in proximity. We introduce diffusion state distance (DSD), a new metric based on a graph diffusion property, designed to capture finer-grained distinctions in proximity for transfer of functional annotation in PPI networks. We present a tool that, when input a PPI network, will output the DSD distances between every pair of proteins. We show that replacing the shortest-path metric by DSD improves the performance of classical function prediction methods across the board.},
    author = {Caao, Mengfei and Zhang, Hao and Park, Jisoo and Daniels, Noah M. and Crovella, Mark E. and Cowen, Lenore J. and Hescott, Benjamin},
    doi = {10.1371/journal.pone.0076339},
    issn = {19326203},
    journal = {PLoS ONE},
    number = {10},
    pages = {1--12},
    pmid = {24194834},
    title = {{Going the Distance for Protein Function Prediction: A New Distance Metric for Protein Interaction Networks}},
    volume = {8},
    year = {2013}
}
       
@report{Luxburg,
    author = {Luxburg, Ulrike Von},
    number = {August},
    title = {{A Tutorial on Spectral Clustering}},
    year = {2006}
}

@article{NodeEmbed,
    abstract = {Neural node embeddings have recently emerged as a powerful representation for supervised learning tasks involving graph-structured data. We leverage this recent advance to develop a novel algorithm for unsupervised community discovery in graphs. Through extensive experimental studies on simulated and real-world data, we demonstrate that the proposed approach consistently improves over the current state-of-the-art. Specifically, our approach empirically attains the information-theoretic limits for community recovery under the benchmark Stochastic Block Models for graph generation and exhibits better stability and accuracy over both Spectral Clustering and Acyclic Belief Propagation in the community recovery limits.},
    archivePrefix = {arXiv},
    arxivId = {1611.03028},
    author = {Ding, Weicong and Lin, Christy and Ishwar, Prakash},
    eprint = {1611.03028},
    pages = {1--10},
    title = {{Node Embedding via Word Embedding for Network Community Discovery}},
    journal   = {CoRR},
    url = {http://arxiv.org/abs/1611.03028},
    year = {2016}
}

@article{LevyGoldberg,
    abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
    archivePrefix = {arXiv},
    arxivId = {1405.4053},
    author = {Levy, Omer and Goldberg, Yoav},
    doi = {10.1162/153244303322533223},
    eprint = {1405.4053},
    isbn = {9781634393973},
    issn = {10495258},
    journal = {Advances in Neural Information Processing Systems (NIPS)},
    pages = {2177--2185},
    pmid = {1710995},
    title = {{Neural Word Embedding as Implicit Matrix Factorization}},
    url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
    year = {2014}
}

@article{word2vec,
    abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
    journal = {CoRR},
    archivePrefix = {arXiv},
    arxivId = {1310.4546},
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    doi = {10.1162/jmlr.2003.3.4-5.951},
    eprint = {1310.4546},
    issn = {10495258},
    pages = {1--9},
    pmid = {903},
    title = {{Distributed Representations of Words and Phrases and their Compositionality}},
    url = {http://arxiv.org/abs/1310.4546},
    year = {2013}
}

@thesis{Bailey,
    author = {Bailey, Eric},
    title = {{Master's Thesis: Capturing and evaluating higher order relations in word embeddings using tensor factorization}},
    year = {2017}
}
