@article{Luxburg,
    author = {Luxburg, Ulrike Von},
    number = {August},
    title = {{A Tutorial on Spectral Clustering}},
    year = {2006}
}

@article{NodeEmbed,
    abstract = {Neural node embeddings have recently emerged as a powerful representation for supervised learning tasks involving graph-structured data. We leverage this recent advance to develop a novel algorithm for unsupervised community discovery in graphs. Through extensive experimental studies on simulated and real-world data, we demonstrate that the proposed approach consistently improves over the current state-of-the-art. Specifically, our approach empirically attains the information-theoretic limits for community recovery under the benchmark Stochastic Block Models for graph generation and exhibits better stability and accuracy over both Spectral Clustering and Acyclic Belief Propagation in the community recovery limits.},
    archivePrefix = {arXiv},
    arxivId = {1611.03028},
    author = {Ding, Weicong and Lin, Christy and Ishwar, Prakash},
    eprint = {1611.03028},
    pages = {1--10},
    title = {{Node Embedding via Word Embedding for Network Community Discovery}},
    journal   = {CoRR},
    url = {http://arxiv.org/abs/1611.03028},
    year = {2016}
}

@article{LevyGoldberg,
    abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
    archivePrefix = {arXiv},
    arxivId = {1405.4053},
    author = {Levy, Omer and Goldberg, Yoav},
    doi = {10.1162/153244303322533223},
    eprint = {1405.4053},
    isbn = {9781634393973},
    issn = {10495258},
    journal = {Advances in Neural Information Processing Systems (NIPS)},
    pages = {2177--2185},
    pmid = {1710995},
    title = {{Neural Word Embedding as Implicit Matrix Factorization}},
    url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
    year = {2014}
}

@article{word2vec,
    abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
    journal = {CoRR},
    archivePrefix = {arXiv},
    arxivId = {1310.4546},
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    doi = {10.1162/jmlr.2003.3.4-5.951},
    eprint = {1310.4546},
    issn = {10495258},
    pages = {1--9},
    pmid = {903},
    title = {{Distributed Representations of Words and Phrases and their Compositionality}},
    url = {http://arxiv.org/abs/1310.4546},
    year = {2013}
}

@article{Bailey,
    author = {Bailey, Eric},
    title = {{Master's Thesis: Capturing and evaluating higher order relations in word embeddings using tensor factorization}},
    year = {2017}
}

@article{NBT-Ihara,
    abstract = {We study the mixing rate of non-backtracking random walks on graphs by looking at non-backtracking walks as walks on the directed edges of a graph. A result known as Ihara's Theorem relates the adjacency matrix of a graph to a matrix related to non-backtracking walks on the directed edges. We prove a weighted version of Iha-ra's Theorem which relates the transition probability matrix of a non-backtracking walk to the transition matrix for the usual random walk. This allows us to determine the spectrum of the transition probability matrix of a non-backtracking random walk in the case of regular graphs and biregular graphs. As a corollary, we obtain a result of Alon et al. in [1] that in most cases, a non-backtracking random walk on a regular graph has a faster mixing rate than the usual random walk. In addition, we obtain an analogous result for biregular graphs.},
    archivePrefix = {arXiv},
    arxivId = {1603.05553},
    author = {Kempton, Mark},
    doi = {10.4236/ojdm.2016.64018},
    eprint = {1603.05553},
    issn = {2161-7635},
    journal = {Open Journal of Discrete Mathematics},
    keywords = {Graph,Ihara Zeta Identity,Mixing Rate,Non-Backtracking Random Walk,Random Walk},
    pages = {207--226},
    title = {{Non-Backtracking Random Walks and a Weighted Ihara's Theorem}},
    url = {http://www.scirp.org/journal/ojdm{\%}0Ahttp://dx.doi.org/10.4236/ojdm.2016.64018{\%}0Ahttp://creativecommons.org/licenses/by/4.0/},
    volume = {6},
    year = {2016}
}

@article{Alon,
abstract = {We compute the mixing rate of a non-backtracking random walk on a regular expander. Using some properties of Chebyshev polynomials of the second kind, we show that this rate may be up to twice as fast as the mixing rate of the simple random walk. The closer the expander is to a Ramanujan graph, the higher the ratio between the above two mixing rates is. As an application, we show that if G is a high-girth regular expander on n vertices, then a typical non-backtracking random walk of length n on G does not visit a vertex more than {\$}(1 + o(1))\backslashfrac{\{}\backslashlog n{\}}{\{}\backslashlog\backslash,\backslashlog n{\}}{\$} times, and this result is tight. In this sense, the multi-set of visited vertices is analogous to the result of throwing n balls to n bins uniformly, in contrast to the simple random walk on G, which almost surely visits some vertex $\Omega$(log n) times.},
archivePrefix = {arXiv},
arxivId = {math/0610550},
author = {Alon, Noga and Benjamini, Itai and Lbetzsky, Eyal and Sodin, Sasha},
doi = {10.1142/S0219199707002551},
eprint = {0610550},
issn = {0219-1997},
journal = {Communications in Contemporary Mathematics},
keywords = {Non-backtracking random walks,balls and bins,expanders,girth,mixing rate},
number = {04},
pages = {585--603},
primaryClass = {math},
title = {{Non-Backtracking Random Walks Mix Faster}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0219199707002551},
volume = {09},
year = {2007}
}

@article{Lovasz,
abstract = {Various aspects of the theory of random walks on graphs are surveyed. In particular, estimates on the important parameters of access time, commute time, cover time and mixing time are discussed. Connections with the eigenvalues of graphs and with electrical networks, and the use of these connections in the study of random walks is described. We also sketch recent algorithmic applications of random walks, in particular to the problem of sampling.},
author = {Lov{\'{a}}sz, L{\'{a}}szl{\'{o}}},
doi = {10.1.1.39.2847},
isbn = {9789638022738},
issn = {03044149},
journal = {Combinatorics: Paul Erdos is Eighty},
number = {Volume 2},
pages = {1--46},
title = {{Random walks on graphs: A survey}},
url = {http://www.cs.yale.edu/publications/techreports/tr1029.pdf},
volume = {2},
year = {1993}
}

@article{Redemption,
abstract = {Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here we introduce a new class of spectral algorithms based on a non-backtracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all the way down to the theoretical limit. We also show the spectrum of the non-backtracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.},
archivePrefix = {arXiv},
arxivId = {1306.5550},
author = {Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborov{\'{a}}, Lenka and Zhang, Pan},
doi = {10.1073/pnas.1312486110},
eprint = {1306.5550},
isbn = {0027-8424, 1091-6490},
issn = {0027-8424},
pages = {1--11},
pmid = {24277835},
title = {{Spectral redemption: clustering sparse networks}},
url = {http://arxiv.org/abs/1306.5550{\%}0Ahttp://dx.doi.org/10.1073/pnas.1312486110},
year = {2013}
}

@article{Belkin:2001wy,
author = {Belkin, Mikhail and Niyogi, Partha},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering.}},
journal = {NIPS},
year = {2001},
read = {Yes},
rating = {0},
date-added = {2017-10-14T15:48:58GMT},
date-modified = {2017-10-14T16:09:29GMT},
url = {http://dblp.org/rec/conf/nips/BelkinN01},
uri = {\url{papers3://publication/uuid/7E1CDE51-52F4-4254-B573-B1692C6D7618}}
}

@article{Benaim:2011ce,
author = {Bena{\"\i}m, Michel and Tarr{\`e}s, Pierre},
title = {{DYNAMICS OF VERTEX-REINFORCED RANDOM WALKS}},
journal = {The Annals of Probability},
year = {2011},
volume = {39},
number = {6},
pages = {2178--2223},
month = nov,
publisher = {Institute of Mathematical Statistics},
doi = {10.2307/23078757?ref=search-gateway:064435d8a1d1519aef4ecbb6cb00007c},
language = {English},
read = {Yes},
rating = {0},
date-added = {2017-08-10T15:26:09GMT},
date-modified = {2017-10-14T16:09:29GMT},
abstract = {We generalize a result from Volkov [Ann. Probab. 29 (2001) 66{\textendash}91] and prove that, on a large class of locally finite connected graphs of bounded degree (G, ∼) and symmetric reinforcement matrices a = (a i,j ) i,j∈G , the vertex-reinforced random walk (VRRW) eventually localizes with positive probability on subsets which consist of a complete d-partite subgraph with possible loops plus its outer boundary. We first show that, in general, any stable equilibrium of a linear symmetric replicator dynamics with positive payoffs on a graph G satisfies the property that its support is a complete d-partite subgraph of G with possible loops, for some d $\ge$ 1. This result is used here for the study of VRRWs, but also applies to other contexts such as evolutionary models in population genetics and game theory. Next we generalize the result of Pemantle [Probab. Theory Related Fields 92 (1992) 117{\textendash}136] and Bena{\"\i}m [Ann. Probab. 25 (1997) 361{\textendash}392] relating the asymptotic behavior of the VRRW to replicator dynamics. This enables us to conclude that, given any neighborhood of a strictly stable equilibrium with support S, the following event occurs with positive probability: the walk localizes on S $\bigcup$ $\partial$S (where $\partial$S is the outer boundary of S) and the density of occupation of the VRRW converges, with polynomial rate, to a strictly stable equilibrium in this neighborhood.},
uri = {\url{papers3://publication/doi/10.2307/23078757?ref=search-gateway:064435d8a1d1519aef4ecbb6cb00007c}}
}

@article{2013PLoSO...876339C,
author = {Cao, Mengfei and Zhang, Hao and Park, Jisoo and Daniels, Noah M and Crovella, Mark E and Cowen, Lenore J and Hescott, Benjamin},
title = {{Going the Distance for Protein Function Prediction: A New Distance Metric for Protein Interaction Networks}},
journal = {PLoS One},
year = {2013},
volume = {8},
number = {10},
pages = {e76339--},
month = oct,
publisher = {Public Library of Science},
doi = {10.1371/journal.pone.0076339},
language = {English},
read = {Yes},
rating = {0},
date-added = {2017-10-10T15:24:01GMT},
date-modified = {2017-10-14T15:56:07GMT},
abstract = {Not Available},
url = {http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2013PLoSO...876339C&link_type=EJOURNAL},
uri = {\url{papers3://publication/doi/10.1371/journal.pone.0076339}}
}

@article{Coifman:2006un,
author = {Coifman, R R and Lafon, S},
title = {{Diffusion maps}},
journal = {Applied and Computational Harmonic Analysis},
year = {2006},
read = {Yes},
rating = {0},
date-added = {2016-05-06T21:39:37GMT},
date-modified = {2017-10-14T15:51:09GMT},
abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations ...
},
url = {http://www.sciencedirect.com/science/article/pii/S1063520306000546},
uri = {\url{papers3://publication/uuid/9FA4DDDD-4055-483D-A112-3540EDEB5B07}}
}

@article{2017arXiv171003059G,
author = {Garcia-Duran, Alberto and Niepert, Mathias},
title = {{Learning Graph Representations with Embedding Propagation}},
journal = {arXiv.org},
year = {2017},
eprint = {1710.03059v1},
eprinttype = {arxiv},
eprintclass = {1710},
pages = {arXiv:1710.03059},
month = oct,
read = {Yes},
rating = {0},
date-added = {2017-10-14T01:50:23GMT},
date-modified = {2017-10-14T15:58:25GMT},
abstract = {We propose Embedding Propagation (EP), an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.},
url = {http://arxiv.org/abs/1710.03059v1},
uri = {\url{papers3://publication/uuid/F703D09B-D96B-4BC6-B615-BF7DF1E07BA9}}
}

@article{2017arXiv171003059G,
author = {Garcia-Duran, Alberto and Niepert, Mathias},
title = {{Learning Graph Representations with Embedding Propagation}},
journal = {arXiv.org},
year = {2017},
eprint = {1710.03059},
eprinttype = {arxiv},
eprintclass = {cs.LG},
pages = {arXiv:1710.03059},
month = oct,
read = {Yes},
rating = {0},
date-added = {2017-10-13T15:44:07GMT},
date-modified = {2017-10-14T15:56:54GMT},
abstract = {We propose Embedding Propagation (EP), an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.},
url = {http://arxiv.org/abs/1710.03059v1},
uri = {\url{papers3://publication/uuid/9480DF1F-C305-49A3-AABF-580733D25DB6}}
}

@article{Gilmer:2017tl,
author = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
title = {{Neural Message Passing for Quantum Chemistry}},
journal = {arXiv.org},
year = {2017},
eprint = {1704.01212v2},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = apr,
annote = {14 pages},
read = {Yes},
rating = {0},
date-added = {2017-09-26T16:27:25GMT},
date-modified = {2017-10-14T16:09:29GMT},
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
url = {http://arxiv.org/abs/1704.01212v2},
uri = {\url{papers3://publication/uuid/5EAC3D18-2EFD-4CF2-9CCD-85B534A29507}}
}

@article{Gilmer:2017tla,
author = {Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
title = {{Neural Message Passing for Quantum Chemistry.}},
journal = {ICML},
year = {2017},
read = {Yes},
rating = {0},
date-added = {2017-09-26T16:25:58GMT},
date-modified = {2017-10-14T16:09:29GMT},
url = {http://dblp.org/rec/conf/icml/GilmerSRVD17},
uri = {\url{papers3://publication/uuid/B06AE6CF-3DFA-4681-A630-A2F55E18C2C4}}
}

@inproceedings{Grover:2016ex,
author = {Grover, Aditya and Leskovec, Jure},
title = {{node2vec}},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
year = {2016},
pages = {855--864},
publisher = {ACM Press},
address = {New York, New York, USA},
doi = {10.1145/2939672.2939754},
isbn = {9781450342322},
read = {Yes},
rating = {0},
date-added = {2017-10-14T16:17:20GMT},
date-modified = {2017-10-14T16:21:21GMT},
url = {http://dl.acm.org/citation.cfm?doid=2939672.2939754},
uri = {\url{papers3://publication/doi/10.1145/2939672.2939754}}
}

@article{2015arXiv150905808H,
author = {Hashimoto, Tatsunori B and Alvarez-Melis, David and Jaakkola, Tommi S},
title = {{Word, graph and manifold embedding from Markov processes}},
journal = {arXiv.org},
year = {2015},
eprint = {1509.05808},
eprinttype = {arxiv},
eprintclass = {cs.CL},
pages = {arXiv:1509.05808},
month = sep,
read = {Yes},
rating = {0},
date-added = {2017-08-29T20:45:06GMT},
date-modified = {2017-10-14T15:49:41GMT},
abstract = {Continuous vector representations of words and objects appear to carry surprisingly rich semantic content. In this paper, we advance both the conceptual and theoretical understanding of word embeddings in three ways. First, we ground embeddings in semantic spaces studied in cognitive-psychometric literature and introduce new evaluation tasks. Second, in contrast to prior work, we take metric recovery as the key object of study, unify existing algorithms as consistent metric recovery methods based on co-occurrence counts from simple Markov random walks, and propose a new recovery algorithm. Third, we generalize metric recovery to graphs and manifolds, relating co-occurence counts on random walks in graphs and random processes on manifolds to the underlying metric to be recovered, thereby reconciling manifold estimation and embedding algorithms. We compare embedding algorithms across a range of tasks, from nonlinear dimensionality reduction to three semantic language tasks, including analogies, sequence completion, and classification.},
url = {http://arxiv.org/abs/1509.05808v1},
uri = {\url{papers3://publication/uuid/A5A1D261-322E-40AB-B94E-D6174DD5DE9D}}
}

@article{Lawrence:2012tj,
author = {Lawrence, Neil D},
title = {{A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction - Insights and New Models.}},
journal = {Journal of Machine Learning Research},
year = {2012},
read = {Yes},
rating = {0},
date-added = {2017-10-14T15:47:52GMT},
date-modified = {2017-10-14T16:09:29GMT},
url = {http://dblp.org/rec/journals/jmlr/Lawrence12},
uri = {\url{papers3://publication/uuid/EA478C60-A94B-4263-B04D-970FE064180D}}
}

@article{Levy:2014vd,
author = {Levy, Omer and Goldberg, Yoav},
title = {{Neural Word Embedding as Implicit Matrix Factorization}},
journal = {Advances in neural information processing {\ldots}},
year = {2014},
pages = {2177--2185},
read = {Yes},
rating = {0},
date-added = {2016-12-08T17:14:07GMT},
date-modified = {2017-10-14T15:56:38GMT},
abstract = {Abstract We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional ...
},
url = {http://papers.nips.cc/paper/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions},
uri = {\url{papers3://publication/uuid/1F7AB5E2-0BA4-410B-9DAC-55B60B81DE98}}
}

@article{2014arXiv1403.6652P,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {{DeepWalk: Online Learning of Social Representations}},
journal = {arXiv.org},
year = {2014},
eprint = {1403.6652},
eprinttype = {arxiv},
eprintclass = {cs.SI},
pages = {arXiv:1403.6652--710},
month = mar,
address = {New York, New York, USA},
publisher = {ACM Press},
doi = {10.1145/2623330.2623732},
isbn = {9781450329569},
read = {Yes},
rating = {0},
date-added = {2017-10-14T16:19:39GMT},
date-modified = {2017-10-14T16:20:57GMT},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623732},
uri = {\url{papers3://publication/doi/10.1145/2623330.2623732}}
}

@article{Singer:2012tf,
author = {Singer, A and Wu, H T},
title = {{Vector diffusion maps and the connection Laplacian}},
journal = {Communications on pure and applied {\ldots}},
year = {2012},
read = {Yes},
rating = {0},
date-added = {2016-05-26T21:54:59GMT},
date-modified = {2017-10-14T15:52:32GMT},
abstract = {Abstract We introduce vector diffusion maps (VDM), a new mathematical framework for organizing and analyzing massive high-dimensional data sets, images, and shapes. VDMis a mathematical and algorithmic generalization of diffusion maps and other nonlinear ...
},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cpa.21395/abstract},
uri = {\url{papers3://publication/uuid/AADF989F-5424-4E1F-A6D8-D0CB4AFF8B1E}}
}

@article{Talmon:2013ko,
author = {Talmon, Ronen and Cohen, Israel and Gannot, Sharon and Coifman, Ronald R},
title = {{Diffusion Maps for Signal Processing: A Deeper Look at Manifold-Learning Techniques Based on Kernels and Graphs}},
journal = {IEEE Signal Processing Magazine},
year = {2013},
volume = {30},
number = {4},
pages = {75--86},
publisher = {IEEE},
doi = {10.1109/MSP.2013.2250353},
read = {Yes},
rating = {0},
date-added = {2016-05-26T20:38:22GMT},
date-modified = {2017-10-14T15:55:02GMT},
abstract = {Signal processing methods have significantly changed over the last several decades. Traditional methods were usually based on parametric statistical inference and linear filters. These frameworks have helped to develop efficient algorithms that have often been suitable for implementation on digital signal processing (DSP) systems. Over the years, DSP systems have advanced rapidly, and their computational capabilities have been substantially increased. This development has enabled contemporary signal processing algorithms to incorporate more computations. Consequently, we have recently experienced a growing interaction between signal processing and machine-learning approaches, e.g., Bayesian networks, graphical models, and kernel-based methods, whose computational burden is usually high},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6530788},
uri = {\url{papers3://publication/doi/10.1109/MSP.2013.2250353}}
}

